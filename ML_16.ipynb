{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d664caa6",
   "metadata": {},
   "source": [
    "#### 1. In a linear equation, what is the difference between a dependent variable and an independent variable?\n",
    "\n",
    "- Dependent Variable: The dependent variable, also known as the response variable or the outcome variable, is the variable that is being predicted or explained by the independent variable(s). It is the variable whose value depends on the values of the independent variable(s). \n",
    "\n",
    "- Independent Variable: The independent variable, also known as the predictor variable or the explanatory variable, is the variable that is manipulated or controlled in the equation. It is the variable that is believed to have an influence or impact on the dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5342ea9",
   "metadata": {},
   "source": [
    "#### 2. What is the concept of simple linear regression? Give a specific example.\n",
    "Simple linear regression is a statistical technique used to model the relationship between two variables: a dependent variable (response variable) and an independent variable (predictor variable) in a linear manner. It assumes a linear relationship between the variables, meaning that the change in the dependent variable is directly proportional to the change in the independent variable.\n",
    "\n",
    "**example for  simple linear regression:**\n",
    "\n",
    "Let's say we want to analyze the relationship between the number of hours studied and the exam scores of a group of students. The number of hours studied is our independent variable (x), and the exam scores are our dependent variable (y).\n",
    "\n",
    "We collect data from 20 students, recording the number of hours each student studied (x) and their corresponding exam scores (y). We can then use simple linear regression to find the best-fitting line that represents the relationship between the two variables.\n",
    "\n",
    "By fitting the data points to a linear equation (y = mx + c), where y represents the exam scores, x represents the number of hours studied, m represents the slope of the line (representing the change in y for a unit change in x), and c represents the y-intercept (the value of y when x is zero), we can estimate the relationship between the number of hours studied and exam scores.\n",
    "\n",
    "Once the regression line is determined, we can use it to make predictions. For example, if a student studies for 5 hours, we can use the regression line to estimate their expected exam score based on the relationship observed in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30024ee9",
   "metadata": {},
   "source": [
    "#### 3. In a linear regression, define the slope.\n",
    "In linear regression, the slope represents the rate of change in the dependent variable (y) per unit change in the independent variable (x). It determines the steepness or the inclination of the regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046fcc90",
   "metadata": {},
   "source": [
    "##### 4. Determine the graph&#39;s slope, where the lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2).\n",
    " slope = (change in y) / (change in x)\n",
    "\n",
    "The change in y is 2 - 2 = 0.\n",
    "\n",
    "The change in x is 3 - 2 = 1.\n",
    "\n",
    "slope = (0) / (1) = 0\n",
    "\n",
    "The slope of the line is 0, which means the line is horizontal and has no incline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc22508f",
   "metadata": {},
   "source": [
    "#### 5. In linear regression, what are the conditions for a positive slope?\n",
    "- The independent variable and dependent variable have a positive correlation: As the values of the independent variable increase, the values of the dependent variable also increase.\n",
    "\n",
    "- The residuals (the differences between the observed and predicted values) have a positive average: The overall tendency is that the observed values are greater than the predicted values, indicating a positive relationship between the variables.\n",
    "\n",
    "- The sum of the cross-products of the independent and dependent variables is positive: When calculating the covariance between the independent and dependent variables, a positive value indicates a positive relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a4dc6",
   "metadata": {},
   "source": [
    "#### 6. In linear regression, what are the conditions for a negative slope?\n",
    "\n",
    "- The independent variable and dependent variable have a negative correlation: As the values of the independent variable increase, the values of the dependent variable decrease.\n",
    "\n",
    "- The residuals (the differences between the observed and predicted values) have a negative average: The overall tendency is that the observed values are smaller than the predicted values, indicating a negative relationship between the variables.\n",
    "\n",
    "- The sum of the cross-products of the independent and dependent variables is negative: When calculating the covariance between the independent and dependent variables, a negative value indicates a negative relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5873e1",
   "metadata": {},
   "source": [
    "#### 7. What is multiple linear regression and how does it work?\n",
    "Multiple linear regression is an extension of simple linear regression that involves more than one independent variable. It is used to model the relationship between a dependent variable and multiple independent variables.\n",
    "\n",
    "In multiple linear regression, the goal is to find the best-fitting linear equation that represents the relationship between the dependent variable and the independent variables. The equation can be written as:\n",
    "\n",
    "Y = β0 + β1X1 + β2X2 + ... + βnXn + ε\n",
    "\n",
    "Where:\n",
    "\n",
    "- Y is the dependent variable,\n",
    "    - X1, X2, ..., Xn are the independent variables,\n",
    "    - β0, β1, β2, ..., βn are the coefficients or slopes that represent the effect of each independent variable on the dependent variable,\n",
    "    - ε is the error term or residual.\n",
    "\n",
    "The multiple linear regression model estimates the values of the coefficients (β0, β1, β2, ..., βn) that minimize the sum of squared residuals, which measures the discrepancy between the observed values of the dependent variable and the predicted values based on the independent variables.\n",
    "\n",
    "To estimate the coefficients, various statistical techniques such as Ordinary Least Squares (OLS) can be used. These techniques analyze the relationships between the variables and find the coefficients that best fit the data.\n",
    "\n",
    "Multiple linear regression allows us to account for the simultaneous influence of multiple independent variables on the dependent variable, making it a powerful tool for understanding complex relationships in data and making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305ffe9d",
   "metadata": {},
   "source": [
    "#### 8. In multiple linear regression, define the number of squares due to error.\n",
    "\n",
    "In multiple linear regression, the sum of squares due to error, often denoted as SSE (Sum of Squares Error), is a measure of the variability or the unexplained variation in the dependent variable that is not accounted for by the independent variables in the regression model.\n",
    "\n",
    "The SSE is calculated as the sum of the squared differences between the actual values of the dependent variable and the predicted values based on the regression model. Mathematically, it can be expressed as:\n",
    "\n",
    "SSE = Σ(y - ŷ)²\n",
    "\n",
    "where:\n",
    "\n",
    "- y represents the actual values of the dependent variable,\n",
    "- ŷ represents the predicted values of the dependent variable based on the regression model,\n",
    "- Σ represents the sum of the squared differences across all data points.\n",
    "\n",
    "The SSE represents the variability that is not explained by the regression model and is essentially the sum of the squared residuals or errors. It is a measure of the model's goodness of fit, and a lower SSE indicates a better fit of the regression model to the data.\n",
    "\n",
    "By minimizing the SSE, we can find the optimal values for the regression coefficients that provide the best fit to the data. This is typically done using optimization techniques such as ordinary least squares (OLS), which minimize the SSE to estimate the coefficients that minimize the overall error between the predicted and actual values of the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df24fb",
   "metadata": {},
   "source": [
    "#### 9. In multiple linear regression, define the number of squares due to regression.\n",
    "\n",
    "In multiple linear regression, the number of squares due to regression refers to the sum of squares of the differences between the predicted values and the mean of the dependent variable. It measures the variability in the dependent variable that is explained by the regression model.\n",
    "\n",
    "To understand the concept of sum of squares due to regression, let's break it down:\n",
    "\n",
    "First, we calculate the mean of the dependent variable (Ȳ), which represents the average value of the observed data points.\n",
    "\n",
    "Next, we use the multiple linear regression model to predict the values of the dependent variable (Ŷ) based on the independent variables (X₁, X₂, ..., Xₚ).\n",
    "\n",
    "For each observed data point, we calculate the difference between the predicted value (Ŷ) and the mean of the dependent variable (Ȳ), squared. This is done for all data points.\n",
    "\n",
    "Finally, we sum up all the squared differences, which gives us the sum of squares due to regression (SSR).\n",
    "\n",
    "Mathematically, the sum of squares due to regression can be represented as:\n",
    "\n",
    "SSR = Σ(Ŷ - Ȳ)²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73cd51",
   "metadata": {},
   "source": [
    "#### 10.In a regression equation, what is multicollinearity?\n",
    "\n",
    "Multicollinearity refers to the presence of high correlation or linear dependency among two or more independent variables in a regression equation. It occurs when there is a strong relationship between the independent variables, making it difficult for the model to distinguish the individual effects of each variable on the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3299e85",
   "metadata": {},
   "source": [
    "#### 11. What is heteroskedasticity, and what does it mean?\n",
    "\n",
    "Heteroskedasticity refers to a pattern in the residuals (the differences between the observed and predicted values) of a regression model where the variability of the residuals is not constant across the range of the independent variables. In other words, the spread or dispersion of the residuals differs systematically as the values of the independent variables change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13e4c1",
   "metadata": {},
   "source": [
    "#### 12. Describe the concept of ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22551216",
   "metadata": {},
   "source": [
    "Ridge regression is a regularization technique used in linear regression to mitigate the problem of multicollinearity and improve the model's stability and accuracy. It is particularly useful when there are high correlations among the independent variables.\n",
    "\n",
    "In standard linear regression, the objective is to minimize the sum of squared residuals (error) between the observed and predicted values. However, when there is multicollinearity, the estimated coefficients can be highly sensitive to small changes in the data. This sensitivity can lead to unstable and unreliable coefficient estimates.\n",
    "\n",
    "Ridge regression addresses this issue by adding a penalty term to the sum of squared residuals. This penalty term is proportional to the square of the magnitude of the coefficients, weighted by a parameter called lambda (λ). The higher the value of λ, the stronger the regularization and the more the coefficients are shrunk towards zero.\n",
    "\n",
    "By adding the penalty term, ridge regression imposes a constraint on the size of the coefficients. This helps to reduce the impact of multicollinearity by shrinking the coefficients towards zero, but without completely eliminating any of them. As a result, ridge regression provides a balance between model simplicity (smaller coefficients) and prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3e186",
   "metadata": {},
   "source": [
    "#### 13. Describe the concept of lasso regression.\n",
    "\n",
    "Lasso regression is a technique used in linear regression to select the most important features and estimate their impact on the target variable. It helps simplify the model and improve its interpretability.\n",
    "\n",
    "In lasso regression, we add a penalty term to the traditional linear regression equation. This penalty term encourages some coefficients (weights) to become exactly zero, effectively removing the corresponding features from the model. This means that lasso regression not only estimates the coefficients but also performs feature selection.\n",
    "\n",
    "The penalty term in lasso regression is based on the sum of the absolute values of the coefficients. By adjusting a parameter called lambda (λ), we control the strength of the penalty. A higher λ value leads to more coefficients being reduced to zero, resulting in a simpler model with fewer features. A lower λ value allows more coefficients to have non-zero values.\n",
    "\n",
    "The benefits of lasso regression are that it automatically selects relevant features and handles situations where there are many potential predictors. It provides a balance between simplicity and accuracy by shrinking some coefficients towards zero and removing irrelevant features.\n",
    "\n",
    "The estimated coefficients in lasso regression tell us the impact of each selected feature on the target variable. A non-zero coefficient indicates that the corresponding feature is important in predicting the target. A zero coefficient means that the feature has been excluded from the model because it doesn't contribute significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c2c6b1",
   "metadata": {},
   "source": [
    "#### 14. What is polynomial regression and how does it work?\n",
    "\n",
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled using a polynomial function. In polynomial regression, the regression equation is not a straight line but a curve that fits the data points more accurately when a linear relationship is not sufficient.\n",
    "\n",
    "The process of polynomial regression involves fitting a polynomial equation to the data by adding higher-order polynomial terms to the regression model. Instead of using a linear equation of the form y = β0 + β1x, polynomial regression includes additional terms like β2x^2, β3x^3, and so on, up to a chosen degree.\n",
    "\n",
    "The steps to perform polynomial regression are as follows:\n",
    "\n",
    "Data Preparation: Collect the data with the independent variable(s) and the corresponding dependent variable.\n",
    "\n",
    "Feature Transformation: Transform the independent variable(s) by creating higher-order polynomial terms up to the desired degree.\n",
    "\n",
    "Model Fitting: Fit the polynomial regression model to the transformed data using methods such as least squares or maximum likelihood estimation.\n",
    "\n",
    "Model Evaluation: Assess the goodness of fit and the statistical significance of the polynomial terms using evaluation metrics and statistical tests.\n",
    "\n",
    "Prediction: Use the fitted polynomial regression model to make predictions on new or unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1261918",
   "metadata": {},
   "source": [
    "#### 15. Describe the basis function.\n",
    "\n",
    "In the context of machine learning and regression analysis, a basis function is a mathematical function that transforms the input data into a higher-dimensional space. It is used to represent a more complex relationship between the independent variables and the dependent variable.\n",
    "\n",
    "The purpose of using basis functions is to capture nonlinear relationships and interactions between variables that cannot be adequately modeled using linear functions. By transforming the input data into a higher-dimensional space, the basis functions allow for more flexibility in fitting the regression model and capturing the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d54cb",
   "metadata": {},
   "source": [
    "#### 16. Describe how logistic regression works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
