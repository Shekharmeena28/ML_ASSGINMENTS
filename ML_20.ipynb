{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d858dd",
   "metadata": {},
   "source": [
    "#### 1. What is the underlying concept of Support Vector Machines?\n",
    "The underlying concept of Support Vector Machines (SVM) is to find an optimal hyperplane that can separate data points of different classes in a high-dimensional space. SVM is a supervised machine learning algorithm used for both classification and regression tasks.\n",
    "\n",
    "The key idea behind SVM is to maximize the margin between the decision boundary (hyperplane) and the nearest data points of each class. The decision boundary is defined by a subset of data points called support vectors, which are the closest points to the decision boundary.\n",
    "\n",
    "The main objectives of SVM are:\n",
    "\n",
    "\n",
    "Maximizing Margin: SVM aims to find a hyperplane that maximizes the distance between the decision boundary and the support vectors. This margin represents the separation between classes and provides a measure of robustness and generalization to unseen data.\n",
    "\n",
    "Non-Linear Transformations: SVM can handle non-linearly separable data by applying a technique called the \"kernel trick.\" By transforming the input features into a higher-dimensional space, SVM can find a linear decision boundary that effectively separates the data in the transformed space.\n",
    "\n",
    "Margin-based Decision: SVM uses a margin-based decision criterion. Instead of simply classifying data points based on their position relative to the decision boundary, SVM focuses on maximizing the margin to achieve better generalization performance.\n",
    "\n",
    "Regularization Parameter: SVM incorporates a regularization parameter (C) to control the balance between maximizing the margin and minimizing the training errors. This parameter allows SVM to handle trade-offs between fitting the training data perfectly and generalizing well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bcd7e",
   "metadata": {},
   "source": [
    "#### 2. What is the concept of a support vector?\n",
    "In the context of Support Vector Machines (SVM), a support vector is a data point that lies closest to the decision boundary (hyperplane) separating the classes. These support vectors are crucial in defining the decision boundary and are used in the formulation of the SVM algorithm.\n",
    "\n",
    "The concept of a support vector arises from the objective of SVM, which aims to maximize the margin between the decision boundary and the data points. The decision boundary is defined by a subset of the training data, and the support vectors are the data points from that subset.\n",
    "\n",
    "Support vectors have the following characteristics:\n",
    "\n",
    "Influence on the Decision Boundary: Support vectors are the key data points that determine the position and orientation of the decision boundary. They lie closest to the decision boundary and contribute to its definition.\n",
    "\n",
    "Margin Boundary: Support vectors define the margin, which is the region between the two parallel hyperplanes that bound the decision boundary. The margin is maximized by selecting the support vectors that are located on or closest to the boundary.\n",
    "\n",
    "Loss Function: Support vectors play a crucial role in the SVM optimization process. They contribute to the computation of the loss function, which measures the extent to which data points violate the margin or are misclassified.\n",
    "\n",
    "Robustness and Generalization: Since support vectors lie closest to the decision boundary, they are considered the most informative and critical instances for classification. They represent the most challenging or ambiguous cases, and SVM focuses on correctly classifying them, leading to better generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4987e99",
   "metadata": {},
   "source": [
    "##### 3. When using SVMs, why is it necessary to scale the inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9777f408",
   "metadata": {},
   "source": [
    "Scaling the inputs is necessary when using Support Vector Machines (SVMs) for several reasons:\n",
    "\n",
    "- Influence of feature scales: SVMs aim to find the optimal hyperplane that separates the data points of different classes. The decision boundary is affected by the scales of the features. If the features have different scales, it can lead to an uneven influence on the decision boundary. Features with larger scales may dominate the optimization process, while features with smaller scales may have negligible impact. Scaling the inputs helps to ensure that all features contribute proportionally to the SVM's decision-making process.\n",
    "\n",
    "- Numerical stability: SVM algorithms involve solving optimization problems, such as finding the maximum-margin hyperplane. The optimization process can be sensitive to the scale of the input features. Features with larger scales can result in larger values in the optimization process, which may lead to numerical instability or convergence issues. By scaling the inputs, you bring all the features to a similar range, avoiding numerical problems during the optimization process.\n",
    "\n",
    "- Kernel functions: SVMs often use kernel functions to transform the input features into higher-dimensional spaces, where the classes may be more separable. Some kernel functions, such as the Radial Basis Function (RBF) kernel, rely on the calculation of distances between data points. When the input features have different scales, the distance calculations may be biased towards the features with larger scales. Scaling the inputs ensures that the kernel functions work effectively and do not favor certain features based on their scales.\n",
    "\n",
    "To address these issues, it is recommended to scale the inputs before training an SVM model. Common scaling techniques include standardization (subtracting the mean and dividing by the standard deviation) or normalization (scaling to a specific range, such as [0, 1]). By scaling the inputs, you ensure that the features contribute equally, promote numerical stability, and enable the kernel functions to work optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a5034",
   "metadata": {},
   "source": [
    "#### 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d76c3",
   "metadata": {},
   "source": [
    "Yes, an SVM classifier can output a confidence score or a probability estimate for its classification decision, depending on the specific implementation or variant of SVM used.\n",
    "\n",
    "In traditional binary SVM classification, the classifier assigns a data point to one of two classes based on which side of the decision boundary it falls. The confidence score represents the distance of the data point from the decision boundary. A higher confidence score indicates a stronger certainty in the classification, while a lower score suggests a lower confidence.\n",
    "\n",
    "However, SVM classifiers don't inherently provide probability estimates like some other classifiers (e.g., logistic regression or Naive Bayes). To obtain probability estimates, additional techniques such as Platt scaling or sigmoid calibration can be applied to convert the SVM's decision scores into probability values. These techniques use a calibration dataset to map the scores to probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f6c60",
   "metadata": {},
   "source": [
    "#### 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
    "\n",
    "When training a model on a training set with millions of instances and hundreds of features, it is generally more efficient to use the dual form of the SVM problem. Here's why:\n",
    "\n",
    "Computational Efficiency: The dual form of the SVM problem is computationally more efficient for large-scale datasets. When the number of instances is large compared to the number of features, the dual form offers advantages in terms of computational complexity and memory usage. It allows for more efficient computations, making it feasible to train the model on such a large dataset.\n",
    "\n",
    "Kernel Trick: If you are using non-linear kernels, such as the Gaussian (RBF) kernel, the dual form is necessary. Kernels enable SVMs to handle complex, non-linear relationships in the data. When dealing with a high-dimensional feature space, the dual form allows the SVM to implicitly operate in this transformed space without explicitly computing the transformations. This avoids the need for potentially expensive and memory-intensive calculations associated with the primal form.\n",
    "\n",
    "Support Vector Selection: The dual form is directly connected to the concept of support vectors, which are the critical data points defining the decision boundary. In large-scale datasets, only a subset of instances becomes support vectors, while the remaining instances do not affect the decision boundary. The dual form facilitates efficient selection and representation of support vectors, reducing the computational burden during training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6b8c0",
   "metadata": {},
   "source": [
    "#### 6. Let&#39;s say you&#39;ve used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e6490",
   "metadata": {},
   "source": [
    "RBF (Radial Basis Function) kernel and it appears to underfit the training data, adjusting the parameters, such as gamma and C, can help improve the performance. Here's how you can approach it:\n",
    "\n",
    "Gamma (γ): The gamma parameter determines the influence of individual training samples on the decision boundary. A higher gamma value makes the decision boundary more complex and can lead to overfitting, where the model becomes too specific to the training data. In contrast, a lower gamma value makes the decision boundary smoother and can result in underfitting, where the model fails to capture the underlying patterns in the data.\n",
    "\n",
    "If the RBF kernel underfits the training data, you should consider raising the gamma value. This allows the model to have a more flexible decision boundary that can better fit the training instances. It helps to increase the influence of individual training samples on the decision-making process.\n",
    "C parameter: The C parameter in SVM controls the trade-off between achieving a larger margin and ensuring that the training instances are correctly classified. A smaller C value allows for a larger margin but may lead to more misclassifications. On the other hand, a larger C value focuses on accurate classification and can lead to overfitting if not properly tuned.\n",
    "\n",
    "If the RBF kernel underfits the training data, you should consider lowering the C value. This encourages a larger margin, allowing the model to generalize better and potentially capture more of the underlying patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa50c0",
   "metadata": {},
   "source": [
    "#### 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "\n",
    "To solve the soft margin linear SVM classifier problem using an off-the-shelf Quadratic Programming (QP) solver, you need to set the QP parameters (H, f, A, and b) appropriately. Here's how you can determine these parameters:\n",
    "\n",
    "H (Quadratic Coefficient Matrix): H is a matrix that represents the quadratic coefficients of the objective function in the QP problem. For a soft margin linear SVM, H is typically an identity matrix or a diagonal matrix with all diagonal elements as 1. This indicates that the objective function is a sum of squared weights and encourages smaller weights for regularization.\n",
    "\n",
    "f (Linear Coefficient Vector): f is a vector that represents the linear coefficients of the objective function in the QP problem. It is derived from the regularization term and the misclassification errors. The values of f depend on the specific SVM formulation and the soft margin constraint. It is usually set accordingly to incorporate regularization and misclassification penalties.\n",
    "\n",
    "A (Constraint Coefficient Matrix): A is a matrix that represents the coefficients of the inequality constraints in the QP problem. For a soft margin linear SVM, the constraints are related to the margin and the misclassifications. Each row of A corresponds to a data point and contains the feature vector multiplied by the class label (positive or negative). The matrix A is constructed by stacking these rows for all data points.\n",
    "\n",
    "b (Constraint Vector): b is a vector that represents the right-hand side of the inequality constraints in the QP problem. For a soft margin linear SVM, the constraints involve the margin and misclassification penalties. The values of b depend on the specific SVM formulation and the soft margin constraint. It is typically set to the appropriate margins and misclassification penalty values.\n",
    "\n",
    "Once you have set the QP parameters (H, f, A, and b) correctly, you can feed them into the off-the-shelf QP solver to find the optimal solution for the soft margin linear SVM classifier problem`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4098f9f",
   "metadata": {},
   "source": [
    "#### 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daef5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a05dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee949d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8758a087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91089e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa31a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, y_train)\n",
    "linear_svc_predictions = linear_svc.predict(X_test)\n",
    "linear_svc_accuracy = accuracy_score(y_test, linear_svc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e9c22f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "svc_predictions = svc.predict(X_test)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0d32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='hinge')\n",
    "sgd.fit(X_train, y_train)\n",
    "sgd_predictions = sgd.predict(X_test)\n",
    "sgd_accuracy = accuracy_score(y_test, sgd_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e220f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC Accuracy: 0.9\n",
      "SVC Accuracy: 0.925\n",
      "SGDClassifier Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "print(\"LinearSVC Accuracy:\", linear_svc_accuracy)\n",
    "print(\"SVC Accuracy:\", svc_accuracy)\n",
    "print(\"SGDClassifier Accuracy:\", sgd_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77daeaab",
   "metadata": {},
   "source": [
    "##### 9. On the MNIST dataset, train an SVM classifier. You&#39;ll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0de0a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d2a30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "y = mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6afdd6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "1          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "2          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "3          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "...        ...  ...       ...       ...       ...       ...       ...   \n",
       "69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0           0.0       0.0       0.0       0.0       0.0  \n",
       "1           0.0       0.0       0.0       0.0       0.0  \n",
       "2           0.0       0.0       0.0       0.0       0.0  \n",
       "3           0.0       0.0       0.0       0.0       0.0  \n",
       "4           0.0       0.0       0.0       0.0       0.0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "69995       0.0       0.0       0.0       0.0       0.0  \n",
       "69996       0.0       0.0       0.0       0.0       0.0  \n",
       "69997       0.0       0.0       0.0       0.0       0.0  \n",
       "69998       0.0       0.0       0.0       0.0       0.0  \n",
       "69999       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[70000 rows x 784 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baaf7448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        5\n",
       "1        0\n",
       "2        4\n",
       "3        1\n",
       "4        9\n",
       "        ..\n",
       "69995    2\n",
       "69996    3\n",
       "69997    4\n",
       "69998    5\n",
       "69999    6\n",
       "Name: class, Length: 70000, dtype: category\n",
       "Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b766512",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0aca557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(decision_function_shape='ovr')\n",
    "# Perform hyperparameter tuning using a small validation set\n",
    "X_train_small, X_val, y_train_small, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "# Train the SVM classifier on the validation set\n",
    "svm.fit(X_train_small, y_train_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf5ca6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm.predict(X_test)\n",
    "precision = precision_score(y_test, svm_predictions, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6289bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9753773890284089\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46805df",
   "metadata": {},
   "source": [
    "#### 10. On the California housing dataset, train an SVM regressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "074091fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d770a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0d89ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd8dd23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_regressor = SVR()\n",
    "svm_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14d105d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2deaf675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.3320115421348737\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37edb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
